# confirm the driver picked up the 10GB nic
┌─[root@zfs] - [/root] - [2016-01-03 09:11:14]
└─[0] <> dladm show-phys
LINK         MEDIA                STATE      SPEED  DUPLEX    DEVICE
e1000g0      Ethernet             up         1000   full      e1000g0
ixgbe0       Ethernet             up         10000  full      ixgbe0

# then assign an IP to the 10gb Nic

┌─[root@zfs] - [/root] - [2016-01-03 09:36:13]
└─[1] <> ipadm create-if ixgbe0  
┌─[root@zfs] - [/root] - [2016-01-03 09:36:27]
└─[0] <> ipadm create-addr -T static -a 10.1.0.3/24 ixgbe0/v4static
┌─[root@zfs] - [/root] - [2016-01-03 09:36:59]
└─[0] <> ping 10.1.0.4
10.1.0.4 is alive

# the 10.1.0.4 is the MAC

Try an iperf

# initially looked like this

┌─[elatov@macair] - [/Users/elatov] - [2016-01-03 09:50:01]
└─[0] <> iperf -c 10.1.0.3 -w 2m -t 30
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size: 2.00 MByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
[  4] local 10.1.0.4 port 49310 connected with 10.1.0.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0-30.0 sec  25.7 GBytes  7.36 Gbits/sec


### Trying out jumbo frames
┌─[root@zfs] - [/root/web-gui/data_0.9f5/tools/iperf] - [2016-01-03 09:54:14]
└─[127] <> ipadm delete-if ixgbe0
┌─[root@zfs] - [/root/web-gui/data_0.9f5/tools/iperf] - [2016-01-03 09:54:18]
└─[0] <> dladm set-linkprop -t -p mtu=9000 ixgbe0
┌─[root@zfs] - [/root/web-gui/data_0.9f5/tools/iperf] - [2016-01-03 09:54:22]
└─[0] <> ipadm create-if ixgbe0                                    
┌─[root@zfs] - [/root/web-gui/data_0.9f5/tools/iperf] - [2016-01-03 09:54:35]
└─[0] <> ipadm create-addr -T static -a 10.1.0.3/24 ixgbe0/v4static
┌─[root@zfs] - [/root/web-gui/data_0.9f5/tools/iperf] - [2016-01-03 09:54:42]
└─[0] <> 

# confirm the jumbo frames are enabled:

┌─[root@zfs] - [/root] - [2016-01-03 09:55:41]
└─[0] <> dladm show-link
LINK        CLASS     MTU    STATE    BRIDGE     OVER
e1000g0     phys      1500   up       --         --
ixgbe0      phys      9000   up       --         --


# on the linux machine
┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:26:03]
└─[0] <> ethtool enp10s0f1 
Settings for enp10s0f1:
	Supported ports: [ TP ]
	Supported link modes:   100baseT/Full 
	                        1000baseT/Full 
	                        10000baseT/Full 
	Supported pause frame use: No
	Supports auto-negotiation: Yes
	Advertised link modes:  100baseT/Full 
	                        1000baseT/Full 
	                        10000baseT/Full 
	Advertised pause frame use: No
	Advertised auto-negotiation: Yes
	Speed: 10000Mb/s
	Duplex: Full
	Port: Twisted Pair
	PHYAD: 0
	Transceiver: external
	Auto-negotiation: on
	MDI-X: Unknown
Cannot get wake-on-lan settings: Operation not permitted
	Current message level: 0x00000007 (7)
			       drv probe link
	Link detected: yes
┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:27:10]
└─[0] <> ethtool -i enp10s0f1
driver: ixgbe
version: 4.0.1-k
firmware-version: 0x80000518
expansion-rom-version: 
bus-info: 0000:0a:00.1
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: no

# give it an ip

sudo ifconfig enp10s0f1 10.1.0.5 netmask 255.255.255.0 up

# then 

# actually doing a dual test show the full pipe

┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:31:11]
└─[0] <> iperf -c 10.1.0.3 -w 2m -d  
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size:  416 KByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size:  416 KByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
[  5] local 10.1.0.5 port 42934 connected with 10.1.0.3 port 5001
[  4] local 10.1.0.5 port 5001 connected with 10.1.0.3 port 53296
[ ID] Interval       Transfer     Bandwidth
[  5]  0.0-10.0 sec  4.69 GBytes  4.03 Gbits/sec
[  4]  0.0-10.0 sec  7.59 GBytes  6.52 Gbits/sec

# with parallel able to get same bw
┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:34:23]
└─[0] <> iperf -c 10.1.0.3 -w 2m -t 15 -P 2
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size:  416 KByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
[  3] local 10.1.0.5 port 42992 connected with 10.1.0.3 port 5001
[  4] local 10.1.0.5 port 42994 connected with 10.1.0.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-15.0 sec  6.59 GBytes  3.78 Gbits/sec
[  4]  0.0-15.0 sec  6.60 GBytes  3.78 Gbits/sec
[SUM]  0.0-15.0 sec  13.2 GBytes  7.56 Gbits/sec

Try with Jumbo frames

┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:50:52]
└─[0] <> sudo ip link set enp10s0f1 mtu 9000

confirm they are working

┌─[elatov@air] - [/home/elatov] - [2016-01-03 10:52:23]
└─[0] <> ping -s 9000 10.1.0.3
PING 10.1.0.3 (10.1.0.3) 9000(9028) bytes of data.
9008 bytes from 10.1.0.3: icmp_seq=2 ttl=255 time=0.575 ms
9008 bytes from 10.1.0.3: icmp_seq=3 ttl=255 time=0.240 ms
9008 bytes from 10.1.0.3: icmp_seq=4 ttl=255 time=0.264 ms
^C
--- 10.1.0.3 ping statistics ---
4 packets transmitted, 3 received, 25% packet loss, time 3008ms
rtt min/avg/max/mdev = 0.240/0.359/0.575/0.154 ms

# changes on the solaris machine
ndd -set /dev/ip ip_lso_outbound 0
ipadm set-prop -p max_buf=4194304 tcp
ipadm set-prop -p recv_buf=1048576 tcp
ipadm set-prop -p send_buf=1048576 tcp

## Sites
https://elkano.org/blog/testing-10g-network-iperf/
http://hardforum.com/archive/index.php/t-1769829.html
http://blog.cyberexplorer.me/2013/03/improving-vm-to-vm-network-throughput.html
https://www.myricom.com/solutions/10-gigabit-ethernet/performance.html#solaris_gldv3
https://www.myricom.com/software/myri10ge/433-how-do-i-achieve-the-highest-possible-bandwidth-with-solaris-myri10ge.html
http://slaptijack.com/system-administration/solaris-tcp-performance-tuning/
http://unixadminschool.com/blog/2012/08/solaris-network-performance-tuning-know-about-tcp-window-size/
https://www.myricom.com/software/myri10ge/392-how-do-i-troubleshoot-slow-myri10ge-or-mx-10g-performance.html
http://networking-discuss.opensolaris.narkive.com/xW3bNLL5/opensolaris-low-latency-networking

# try flow control
dladm set-linkprop -t -p flowctrl=3 ixgbe0

# with flow control and jumbo frames

┌─[elatov@air] - [/home/elatov] - [2016-01-03 11:33:32]
└─[1] <> iperf -c 10.1.0.3 -m -w 2m          
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size:  416 KByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
[  3] local 10.1.0.5 port 43882 connected with 10.1.0.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  9.18 GBytes  7.89 Gbits/sec
[  3] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)

# quick NFS test
┌─[elatov@air] - [/mnt/nfs] - [2016-01-03 01:08:58]
└─[0] <> dd if=/dev/zero of=test.dd bs=1M count=1000
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 3.5664 s, 294 MB/s


### after disable intr_polling

ndd -set /dev/ixgbe0 intr_throttling 1

─[elatov@air] - [/home/elatov] - [2016-01-03 03:55:00]
└─[0] <> netperf -H 10.1.0.3 -t TCP_STREAM -C -c -l 60  -- -s 512K -S 512K
MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.1.0.3 () port 0 AF_INET : demo
Recv   Send    Send                          Utilization       Service Demand
Socket Socket  Message  Elapsed              Send     Recv     Send    Recv
Size   Size    Size     Time     Throughput  local    remote   local   remote
bytes  bytes   bytes    secs.    10^6bits/s  % S      % M      us/KB   us/KB

524744 425984 425984    60.00      8114.30   4.48     31.14    0.181   1.257 


### esxi test
[root@macm:~] /usr/lib/vmware/vsan/bin/iperf -c 10.1.0.3 -P 4
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size: 35.0 KByte (default)
------------------------------------------------------------
[  3] local 10.1.0.10 port 11558 connected with 10.1.0.3 port 5001
[  6] local 10.1.0.10 port 48897 connected with 10.1.0.3 port 5001
[  5] local 10.1.0.10 port 38826 connected with 10.1.0.3 port 5001
[  4] local 10.1.0.10 port 15985 connected with 10.1.0.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.92 GBytes  1.65 Gbits/sec
[  6]  0.0-10.0 sec  5.98 GBytes  5.13 Gbits/sec
[  5]  0.0-10.0 sec  2.04 GBytes  1.75 Gbits/sec
[  4]  0.0-10.0 sec  2.03 GBytes  1.74 Gbits/sec
[SUM]  0.0-10.0 sec  12.0 GBytes  10.3 Gbits/sec

If I ran 1 it only got about 7GB:

[root@macm:~] /usr/lib/vmware/vsan/bin/iperf -c 10.1.0.3 -w 2m -m
------------------------------------------------------------
Client connecting to 10.1.0.3, TCP port 5001
TCP window size: 2.01 MByte (WARNING: requested 2.00 MByte)
------------------------------------------------------------
[  3] local 10.1.0.10 port 56137 connected with 10.1.0.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  8.02 GBytes  6.89 Gbits/sec
[  3] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)

## During small io test
┌─[root@zfs] - [/root] - [2016-01-09 09:38:30]
└─[0] <> /var/web-gui/data_last/tools/nicstat/nicstat -M -i ixgbe0 1
    Time      Int   rMbps   wMbps   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
09:38:32   ixgbe0    0.00    0.00    0.06    0.04  3209.1   701.5  0.00   0.00
09:38:33   ixgbe0  4452.6   30.03 65822.6 46772.0  8866.4   84.15  46.7   0.00
09:38:34   ixgbe0  5930.6   39.35 86300.8 61054.4  9007.3   84.49  62.2   0.00
09:38:35   ixgbe0  6280.4   41.69 91414.3 64683.4  9005.0   84.48  65.9   0.00
09:38:36   ixgbe0  6345.0   42.16 92388.5 65413.3  9001.7   84.49  66.5   0.00
09:38:37   ixgbe0  6135.4   40.72 89383.0 63171.0  8997.1   84.48  64.3   0.00
09:38:38   ixgbe0  4924.7   33.01 72629.9 51337.1  8887.5   84.29  51.6   0.00
09:38:39   ixgbe0  6347.3   42.14 92361.6 65384.3  9007.6   84.48  66.6   0.00

### iperf for esxi
https://vibsdepot.v-front.de/wiki/index.php/Iperf

## install it
http://nilic.github.io/2015/01/15/downloading-files-with-wget-on-esxi/
esxcli software vib install -v /tmp/iperf-2.0.5-1.x86_64.vib --no-sig-check

### Enable mpio
# let's change the LUn to use RR
# first figure out the available PSPs:
[root@macm:~] esxcli storage nmp satp list
Name                 Default PSP    Description
-------------------  -------------  -------------------------------------------------------
VMW_SATP_ALUA        VMW_PSP_MRU    Supports non-specific arrays that use the ALUA protocol
VMW_SATP_MSA         VMW_PSP_MRU    Placeholder (plugin not loaded)
VMW_SATP_DEFAULT_AP  VMW_PSP_MRU    Placeholder (plugin not loaded)
VMW_SATP_SVC         VMW_PSP_FIXED  Placeholder (plugin not loaded)
VMW_SATP_EQL         VMW_PSP_FIXED  Placeholder (plugin not loaded)
VMW_SATP_INV         VMW_PSP_FIXED  Placeholder (plugin not loaded)
VMW_SATP_EVA         VMW_PSP_FIXED  Placeholder (plugin not loaded)
VMW_SATP_ALUA_CX     VMW_PSP_RR     Placeholder (plugin not loaded)
VMW_SATP_SYMM        VMW_PSP_RR     Placeholder (plugin not loaded)
VMW_SATP_CX          VMW_PSP_MRU    Placeholder (plugin not loaded)
VMW_SATP_LSI         VMW_PSP_MRU    Placeholder (plugin not loaded)
VMW_SATP_DEFAULT_AA  VMW_PSP_FIXED  Supports non-specific active/active arrays
VMW_SATP_LOCAL       VMW_PSP_FIXED  Supports direct attached devices

Then check what the current one is set to:

[root@macm:~] esxcli  storage nmp device list -d naa.600144f009d4d236000056918f920002
naa.600144f009d4d236000056918f920002
   Device Display Name: SUN iSCSI Disk (naa.600144f009d4d236000056918f920002)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on; action_OnRetryErrors=off; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_MRU
   Path Selection Policy Device Config: Current Path=vmhba37:C0:T0:L4
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba37:C0:T0:L4
   Is USB: false

# you will notice the default is MRU (Most recently used)
# if we want to use both paths at the same time we can enable RR:
[root@macm:~] esxcli storage nmp device set -d naa.600144f009d4d236000056918f920002 -P VMW_PSP_RR
[root@macm:~] esxcli  storage nmp device list -d naa.600144f009d4d236000056918f920002
naa.600144f009d4d236000056918f920002
   Device Display Name: SUN iSCSI Disk (naa.600144f009d4d236000056918f920002)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on; action_OnRetryErrors=off; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1000,bytes=10485760,useANO=0; lastPathIndex=1: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba37:C1:T0:L4, vmhba37:C0:T0:L4
   Is USB: false

# You will also notice that the Round Robin occurs after 1000 iops as per 
https://nexenta.com/sites/default/files/docs/Best%20Practices%20for%20VMware%20vSphere%20with%20NexentaStor.pdf
http://kb.vmware.com/kb/2069356

We can change that to one, first confirm the setting:

[root@macm:~] esxcli storage nmp psp roundrobin deviceconfig get -d naa.600144f009d4d236000056918f920002
   Byte Limit: 10485760
   Device: naa.600144f009d4d236000056918f920002
   IOOperation Limit: 1000
   Limit Type: Default
   Use Active Unoptimized Paths: false

Then to change it you can do the following:

[root@macm:~] esxcli storage nmp psp roundrobin deviceconfig set -d naa.600144f009d4d236000056918f920002 -I 1 -t iops
[root@macm:~] esxcli storage nmp psp roundrobin deviceconfig get -d naa.600144f009d4d236000056918f920002
   Byte Limit: 10485760
   Device: naa.600144f009d4d236000056918f920002
   IOOperation Limit: 1
   Limit Type: Iops
   Use Active Unoptimized Paths: false

# these settings are done on the fly and after reboot will go away. If we want to make it permanent. We can create a new SATP rule to match on our vendor and model and assign it to our custom SATP. To figure out what our model and vendor 
settings are for a LUN, just run the following:

[root@macm:~] esxcli storage core device list -d naa.600144f009d4d236000056918f920002 | head
naa.600144f009d4d236000056918f920002
   Display Name: SUN iSCSI Disk (naa.600144f009d4d236000056918f920002)
   Has Settable Display Name: true
   Size: 307200
   Device Type: Direct-Access
   Multipath Plugin: NMP
   Devfs Path: /vmfs/devices/disks/naa.600144f009d4d236000056918f920002
   Vendor: SUN
   Model: COMSTAR
   Revision: 1.0

# now to create our SATP rule:

[root@macm:~] esxcli storage nmp satp rule add -V SUN -M COMSTAR -P VMW_PSP_RR -s VMW_SATP_ALUA -e "OmniOS Devices" -O "iops=1"

# iops url
https://timsvirtualworld.com/2015/01/how-to-set-round-robin-psps-default-iops-limit-to-1-for-emc-symmetrix-arrays/

# and then we can confirm the rule is there

[root@macm:~] esxcli storage nmp satp rule list | grep -E '^Name|OmniOS'
Name           Device  Vendor   Model  Driver  Transport  Options Rule Group  Claim Options   Default PSP  PSP Options  Description
VMW_SATP_ALUA  		   SUN      COMSTAR                           user                        VMW_PSP_RR   iops=1       OmniOS Devices

# now let's down all the VMs
[root@macm:~] vmware-autostart.sh stop
# put the host in maintance mode
[root@macm:~] esxcli system maintenanceMode set -e true
# and finally reboot
[root@macm:~] esxcli system shutdown reboot -r "RR"

# then after the reboot make sure all the LUNs are set apppropriately:

[root@macm:~] for disk in $(esxcli  storage nmp device list | grep SUN -B 1 | grep ^naa); do echo ""; esxcli storage nmp psp roundrobin deviceconfig get -d $disk; do
ne

   Byte Limit: 10485760
   Device: naa.600144f009d4d2360000569169d20001
   IOOperation Limit: 1
   Limit Type: Iops
   Use Active Unoptimized Paths: false

   Byte Limit: 10485760
   Device: naa.600144f009d4d23600005588ed0e0002
   IOOperation Limit: 1
   Limit Type: Default
   Use Active Unoptimized Paths: false

   Byte Limit: 10485760
   Device: naa.600144f009d4d2360000564773790001
   IOOperation Limit: 1
   Limit Type: Default
   Use Active Unoptimized Paths: false

   Byte Limit: 10485760
   Device: naa.600144f009d4d236000056918f920002
   IOOperation Limit: 1
   Limit Type: Iops
   Use Active Unoptimized Paths: false

### Thunderbolt delivers ~8gb/s
http://www.macworld.com/article/1158145/thunderbolt_what_you_need_to_know.html
http://www.anandtech.com/show/4489/promise-pegasus-r6-mac-thunderbolt-review/6
http://www.tested.com/tech/457440-theoretical-vs-actual-bandwidth-pci-express-and-thunderbolt/
en4: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	options=2b<RXCSUM,TXCSUM,VLAN_HWTAGGING,TSO4>
	ether 00:30:93:0c:05:c7
	inet6 fe80::230:93ff:fe0c:5c7%en4 prefixlen 64 scopeid 0x9
	inet 10.1.0.4 netmask 0xffffff00 broadcast 10.1.0.255
	nd6 options=1<PERFORMNUD>
	media: autoselect (10GbaseT <full-duplex,flow-control>)
	status: active

─[elatov@macair] - [/Users/elatov] - [2016-01-03 09:56:47]
└─[0] <> ifconfig en4
en4: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 9000
	options=2b<RXCSUM,TXCSUM,VLAN_HWTAGGING,TSO4>
	ether 00:30:93:0c:05:c7
	inet6 fe80::230:93ff:fe0c:5c7%en4 prefixlen 64 scopeid 0x9
	inet 10.1.0.4 netmask 0xffffff00 broadcast 10.1.0.255
	nd6 options=1<PERFORMNUD>
	media: 10GbaseT <full-duplex,flow-control>
	status: active
